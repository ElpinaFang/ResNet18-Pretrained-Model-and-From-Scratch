{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10208,
     "status": "ok",
     "timestamp": 1671335229306,
     "user": {
      "displayName": "Elpina Fang",
      "userId": "06313423315987558468"
     },
     "user_tz": -420
    },
    "id": "qstJt8UcFvPJ",
    "outputId": "0d202734-a577-4509-af6b-cf4239ddfcfc"
   },
   "outputs": [],
   "source": [
    "!pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 37476,
     "status": "ok",
     "timestamp": 1671335302198,
     "user": {
      "displayName": "Elpina Fang",
      "userId": "06313423315987558468"
     },
     "user_tz": -420
    },
    "id": "NMwVG3QbHPXm",
    "outputId": "1642993b-524f-4295-fa9d-b0156e73b39d"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kqgnUsPjueRT"
   },
   "source": [
    "### Import library\n",
    "Pertama kami akan mengimport library dasar seperti numpy, pandas, matplotlib, dsb untuk memenuhi kebutuhan saat pre-processing ataupun modeling. Selain itu library pytorch akan digunakan untuk modelling, sesuai dengan materi perkuliahan Deep Learning yang kami dapatkan. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 500,
     "status": "ok",
     "timestamp": 1671343619414,
     "user": {
      "displayName": "Elpina Fang",
      "userId": "06313423315987558468"
     },
     "user_tz": -420
    },
    "id": "SQC6S1LjdFSx"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import random\n",
    "import time\n",
    "import copy\n",
    "import seaborn as sn\n",
    "import seaborn as sns\n",
    "import warnings as ws\n",
    "ws.filterwarnings('ignore')\n",
    "#--------------------------------------------------------\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms,models\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "from IPython.core.display import HTML,display\n",
    "from PIL import Image\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "import optuna\n",
    "random_seed = 123\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "import numpy as np\n",
    "np.random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eQGVugS0dFS0"
   },
   "source": [
    "### Checking GPU Availability\n",
    "Secara default, tensor diproses di CPU dan model juga diinisialisasi pada CPU. Oleh karena itu, kita harus secara manual memastikan bahwa operasi dilakukan dengan menggunakan GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1126,
     "status": "ok",
     "timestamp": 1671335319822,
     "user": {
      "displayName": "Elpina Fang",
      "userId": "06313423315987558468"
     },
     "user_tz": -420
    },
    "id": "Vgc2-KcFdFS0",
    "outputId": "f1ec9bd9-e80c-4f1d-84dd-8d064a1521db"
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "    print(\"Using GPU\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"Using CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4615,
     "status": "ok",
     "timestamp": 1671335327664,
     "user": {
      "displayName": "Elpina Fang",
      "userId": "06313423315987558468"
     },
     "user_tz": -420
    },
    "id": "je0LU6ppK2gh"
   },
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "with ZipFile('/content/drive/MyDrive/Dataset/Vegetable Images.zip') as zipObj:\n",
    "  zipObj.extractall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fzVTKLyPdFS2"
   },
   "source": [
    "### Walk through Input Directory\n",
    "Kami menggunkan metode os.walk untuk membaca file dataset secara berulang kali"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 830,
     "status": "ok",
     "timestamp": 1671335333179,
     "user": {
      "displayName": "Elpina Fang",
      "userId": "06313423315987558468"
     },
     "user_tz": -420
    },
    "id": "OdfBZw4ZdFS3"
   },
   "outputs": [],
   "source": [
    "def walk_through_dir(directory_name):\n",
    "    for dirpaths,dirnames,filenames in os.walk(directory_name):\n",
    "        text=f\"There are {len(dirnames)} directories and {len(filenames)} images in '{dirpaths}'\"\n",
    "        print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1671335337632,
     "user": {
      "displayName": "Elpina Fang",
      "userId": "06313423315987558468"
     },
     "user_tz": -420
    },
    "id": "wCUDLTvadFS5",
    "outputId": "cd316729-da61-4983-b505-18d7bfc207b4"
   },
   "outputs": [],
   "source": [
    "walk_through_dir('/content/Vegetable Images')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-iAFIM6xdFS6"
   },
   "source": [
    "Dataset vegetable image sebenarnya terdiri dari 15 kelas, yaitu bean, bitter gourd, bottle gourd, brinjal, broccoli, cabbage, capsicum, carrot, cauliflower, cucumber, papaya, potato, pumpkin, radish dan tomato. Gambar pada dataset ini berukuran sama yaitu 224 x 224 pixel berformat .jpg.\n",
    "Namun sesuai petunjuk soal, data yang akan digunakan adalah **carrot**, **papaya**, dan **potato**.\n",
    "Dari output di atas dapat dilihat untuk jumlah images pada masing-masing path. Terdapat masing-masing 1000 images untuk kelas carrot, papaya, potato pada data train, dan terdapat masing-masing 200 images untuk kelas carrot, papaya, potato pada data validation dan data test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z21iyWxddFS8"
   },
   "source": [
    "Menginisialisasi nama variable untuk direktori data train, data validasi dan data testing, serta pendefinisikan show_image function untuk melihat sampel data carrot, papaya dan potato pada data train dan data validation serta size dari imagesnya."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1671335344754,
     "user": {
      "displayName": "Elpina Fang",
      "userId": "06313423315987558468"
     },
     "user_tz": -420
    },
    "id": "DRZsNusgdFS9"
   },
   "outputs": [],
   "source": [
    "train_dir='/content/Vegetable Images/train'\n",
    "val_dir='/content/Vegetable Images/validation'\n",
    "test_dir='/content/Vegetable Images/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 879,
     "status": "ok",
     "timestamp": 1671335351398,
     "user": {
      "displayName": "Elpina Fang",
      "userId": "06313423315987558468"
     },
     "user_tz": -420
    },
    "id": "XMtSvYwZdFS_"
   },
   "outputs": [],
   "source": [
    "image_formats = [\"png\", \"jpg\"];\n",
    "\n",
    "def show_images(image_files,name): \n",
    "    display(HTML('<H5 style=\"color:blue\"> <b>Sampel Data {} </b></H5><hr>'.format(name)))\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    fig.patch.set_facecolor('xkcd:white')\n",
    "    for i in range(len(image_files)):\n",
    "        plt.subplot(3,3,i+1)    \n",
    "        img=mpimg.imread(image_files[i])\n",
    "        plt.imshow(img)\n",
    "        plt.tight_layout()\n",
    "        plt.axis('off')\n",
    "        plt.title(image_files[i].split(\"/\")[5]+\"\\n\"+\"{}x{}\".format(img.shape[0], img.shape[1])) # nama sayur dan ukuran gambar\n",
    "    plt.show()\n",
    "\n",
    "def list_files(dir):\n",
    "    arr = []\n",
    "    for root, dirs, files in os.walk(dir):\n",
    "            for name in files:\n",
    "                if name.endswith(\".jpg\") or name.endswith(\".png\"):\n",
    "                    arr.append(os.path.join(root, name))\n",
    "                    break\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CcjorJ4ydFTA"
   },
   "source": [
    "### Sampel Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bLEZQWQDdFTA"
   },
   "source": [
    "Berikut adalah sampel data untuk masing-masing kelas (**carrot**, **Papaya**, dan **Potato**) pada data train, validation, dan data test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 913
    },
    "executionInfo": {
     "elapsed": 5482,
     "status": "ok",
     "timestamp": 1671335362895,
     "user": {
      "displayName": "Elpina Fang",
      "userId": "06313423315987558468"
     },
     "user_tz": -420
    },
    "id": "9h1witkzdFTB",
    "outputId": "21d7e4c6-a73c-4763-e980-8394941e8ca5"
   },
   "outputs": [],
   "source": [
    "show_images(list_files(train_dir), \"Train\")\n",
    "show_images(list_files(val_dir), \"Validation\")\n",
    "show_images(list_files(test_dir), \"Test\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DRIRILXfdFTB"
   },
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KFFEf5vndFTC"
   },
   "source": [
    "#### Transformation (Load data to pytorch tensor)\n",
    "Disini kami melakukan beberapa transformasi \n",
    "pada gambar seperti secara random memutar gambar secara horizontal, secara random memutar gambar dengan angle 15 derajat, ataupun melakukan GaussianBlur pada gambar dengan kernel size 5,9 dan lainnya."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1042,
     "status": "ok",
     "timestamp": 1671335368016,
     "user": {
      "displayName": "Elpina Fang",
      "userId": "06313423315987558468"
     },
     "user_tz": -420
    },
    "id": "dnktVpgBdFTC"
   },
   "outputs": [],
   "source": [
    "mean=[0.485,0.456,0.406]\n",
    "std=[0.229,0.224,0.225]\n",
    "\n",
    "transform_ = transforms.Compose([transforms.RandomHorizontalFlip(p=0.5),\n",
    "                                 transforms.RandomRotation(15),\n",
    "                                 transforms.ColorJitter(brightness=0.2,contrast=0.1,hue=0.1,saturation=0.1),\n",
    "                                 transforms.RandomAffine(degrees=15, translate=(0.1,0.1), scale=(1, 2), shear=15),\n",
    "                                 transforms.GaussianBlur(kernel_size=(5,9)),\n",
    "                                 transforms.Resize((224,224)),\n",
    "                                 transforms.ToTensor(),\n",
    "                                 transforms.Normalize(mean,std)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6CxH2G8UdFTD"
   },
   "source": [
    "##### Convert to Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 858,
     "status": "ok",
     "timestamp": 1671335372020,
     "user": {
      "displayName": "Elpina Fang",
      "userId": "06313423315987558468"
     },
     "user_tz": -420
    },
    "id": "PAv9jx_ndFTD"
   },
   "outputs": [],
   "source": [
    "train_dataset=torchvision.datasets.ImageFolder(root=train_dir,transform=transform_)\n",
    "val_dataset=torchvision.datasets.ImageFolder(root=val_dir,transform=transform_)\n",
    "test_dataset=torchvision.datasets.ImageFolder(root=test_dir,transform=transform_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eEirmFdkdFTE"
   },
   "source": [
    "##### Defining Data Loader\n",
    "Data Loader merupakan inti dari perangkat pemrosesan data di PyTorch untuk mempersiapkan data termasuk berbagai metode sampling, komputasi paralel, dan pemrosesan terdistribusi. Disini kami memberikan nilai batch size = 32, dimana akan  dalam 1 batch akan diload sebanyak 32 samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 659,
     "status": "ok",
     "timestamp": 1671335375981,
     "user": {
      "displayName": "Elpina Fang",
      "userId": "06313423315987558468"
     },
     "user_tz": -420
    },
    "id": "nMZSVZTVdFTE"
   },
   "outputs": [],
   "source": [
    "batch_size=32\n",
    "train_dl=DataLoader(train_dataset,batch_size,shuffle=True)\n",
    "val_dl=DataLoader(val_dataset,batch_size,shuffle=True)\n",
    "test_dl=DataLoader(test_dataset,batch_size,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1783,
     "status": "ok",
     "timestamp": 1671335381265,
     "user": {
      "displayName": "Elpina Fang",
      "userId": "06313423315987558468"
     },
     "user_tz": -420
    },
    "id": "LOam8KiIdFTF"
   },
   "outputs": [],
   "source": [
    "dataset_sizes = {'train':len(train_dl.dataset),'valid':len(val_dl.dataset)}\n",
    "dataloaders = {'train':train_dl,'valid':test_dl}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1671335382600,
     "user": {
      "displayName": "Elpina Fang",
      "userId": "06313423315987558468"
     },
     "user_tz": -420
    },
    "id": "exUXulxgdFTF",
    "outputId": "d53d60a0-95d5-494d-c04e-a8629de7bddc"
   },
   "outputs": [],
   "source": [
    "images, labels = next(iter(train_dl))\n",
    "print(\"images-size:\", images.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P-vhvwKgdFTG"
   },
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bn66mVy1dFTG"
   },
   "source": [
    "## A. Resnet18 Pretained Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xfr2NJoKdFTG"
   },
   "source": [
    "Dengan menggunakan ResNet18 pretained model dengan PyTorch. Layer terakhir diubah agar sesuai dengan jumlah kelas yang digunakan yaitu 3 kelas (carrot, papaya, dan potato)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "a023fbbc2f604254981af6a24a59cd49",
      "3b5b0c8e5ea9415c94d2e9cf99f1abce",
      "3b2f5b3ae22546039909292e0a124c86",
      "442ac92aca1845d482eb5961412c86af",
      "51e73d49641a45fcb1f6b4ec78bbf5ca",
      "fb2f8691a79f47738d10fc1bfa0a8c60",
      "ca0c9d49710145edb754a7fa38b54792",
      "369f27c64b61409fba31e092f733fa71",
      "6c5429a3ef614f00a2d468d38d9e44de",
      "41cd48a3d3fc4db1931fdebb38fa469b",
      "6c25dd3c886e4bd295efd9fccc4899c5"
     ]
    },
    "executionInfo": {
     "elapsed": 6427,
     "status": "ok",
     "timestamp": 1671335392737,
     "user": {
      "displayName": "Elpina Fang",
      "userId": "06313423315987558468"
     },
     "user_tz": -420
    },
    "id": "kglMsCN3dFTG",
    "outputId": "ff4eecbe-3e58-42d6-c91c-419d508e9fd6"
   },
   "outputs": [],
   "source": [
    "resnet18_tf = models.resnet18(pretrained=True)\n",
    "resnet18_tf = resnet18_tf.cuda() if device else resnet18_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 917,
     "status": "ok",
     "timestamp": 1671335397244,
     "user": {
      "displayName": "Elpina Fang",
      "userId": "06313423315987558468"
     },
     "user_tz": -420
    },
    "id": "SZpCngrIdFTH",
    "outputId": "e12e0864-312b-4b60-96b1-b2834fec9264"
   },
   "outputs": [],
   "source": [
    "print(f'Resnet18 model summary:\\n{resnet18_tf.named_parameters}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ukTlMJRSAdnp"
   },
   "source": [
    "Selanjutnya dilakukan setting parameter pada learning rate, loss function, dan optimizer. Learning rate yang digunakan adalah 0.001, loss function yang digunakan adalah Cross Entropy loss, dan optimizer yang digunakan adalah SGD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1098,
     "status": "ok",
     "timestamp": 1671335403114,
     "user": {
      "displayName": "Elpina Fang",
      "userId": "06313423315987558468"
     },
     "user_tz": -420
    },
    "id": "6qv3CIXQdFTH"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(resnet18_tf.parameters(), lr=0.0001, momentum=0.9)\n",
    "\n",
    "def accuracy(out, labels):\n",
    "    _,pred = torch.max(out, dim=1)\n",
    "    return torch.sum(pred==labels).item()\n",
    "\n",
    "num_ftrs = resnet18_tf.fc.in_features\n",
    "resnet18_tf.fc = nn.Linear(num_ftrs, 3)\n",
    "resnet18_tf.fc = resnet18_tf.fc.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Y5xGWxxBNi4"
   },
   "source": [
    "Selanjutnya dengan epoch=10 dilakukan proses training model untuk memperoleh best validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 639494,
     "status": "ok",
     "timestamp": 1671336047578,
     "user": {
      "displayName": "Elpina Fang",
      "userId": "06313423315987558468"
     },
     "user_tz": -420
    },
    "id": "kRkQFDsmdFTH",
    "outputId": "4fc13367-cb48-43ef-95f5-f27a8f4bbd58"
   },
   "outputs": [],
   "source": [
    "n_epochs = 10\n",
    "print_every = 10\n",
    "valid_loss_min = np.Inf\n",
    "val_loss = []\n",
    "val_acc = []\n",
    "train_loss = []\n",
    "train_acc = []\n",
    "total_step = len(train_dl)*3\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total=0\n",
    "    print(f'Epoch {epoch}\\n')\n",
    "    for batch_idx, (data_, target_) in enumerate(train_dl):\n",
    "        data_, target_ = data_.to(device), target_.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = resnet18_tf(data_)\n",
    "        loss = criterion(outputs, target_)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _,pred = torch.max(outputs, dim=1)\n",
    "        correct += torch.sum(pred==target_).item()\n",
    "        total += target_.size(0)\n",
    "        if (batch_idx) % 20 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch, n_epochs, batch_idx, total_step, loss.item()))\n",
    "    train_acc.append(100 * correct / total)\n",
    "    train_loss.append(running_loss/total_step)\n",
    "    print(f'\\ntrain-loss: {np.mean(train_loss):.4f}, train-acc: {(100 * correct/total):.4f}')\n",
    "    batch_loss = 0\n",
    "    total_t=0\n",
    "    correct_t=0\n",
    "    with torch.no_grad():\n",
    "        resnet18_tf.eval()\n",
    "        for data_t, target_t in (val_dl):\n",
    "            data_t, target_t = data_t.to(device), target_t.to(device)\n",
    "            outputs_t = resnet18_tf(data_t)\n",
    "            loss_t = criterion(outputs_t, target_t)\n",
    "            batch_loss += loss_t.item()\n",
    "            _,pred_t = torch.max(outputs_t, dim=1)\n",
    "            correct_t += torch.sum(pred_t==target_t).item()\n",
    "            total_t += target_t.size(0)\n",
    "        val_acc.append(100 * correct_t/total_t)\n",
    "        val_loss.append(batch_loss/len(val_dl))\n",
    "        network_learned = batch_loss < valid_loss_min\n",
    "        print(f'validation loss: {np.mean(val_loss):.4f}, validation acc: {(100 * correct_t/total_t):.4f}\\n')\n",
    "        \n",
    "        if network_learned:\n",
    "            valid_loss_min = batch_loss\n",
    "            torch.save(resnet18_tf.state_dict(), 'resnet_tf.pt')\n",
    "            print('Improvement-Detected, save-model')\n",
    "    resnet18_tf.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 928,
     "status": "ok",
     "timestamp": 1671336058586,
     "user": {
      "displayName": "Elpina Fang",
      "userId": "06313423315987558468"
     },
     "user_tz": -420
    },
    "id": "7GRoZwQrdFTH"
   },
   "outputs": [],
   "source": [
    "def plot_learning_curves(train_acc, val_acc, train_loss, val_loss):\n",
    "    fig = plt.figure(figsize=(20, 6))\n",
    "    ax = fig.add_subplot(1, 2, 1)\n",
    "    ax.plot(train_acc, '-o',label='Train Acc.')\n",
    "    ax.plot(val_acc, '--<', label='Validation Acc.')\n",
    "    ax.set_title(\"Train-Validation Accuracy\",size=12)\n",
    "    ax.legend(loc='best',fontsize=12)\n",
    "    ax.set_xlabel('Epoch', size=12)\n",
    "    ax.set_ylabel('Accuracy', size=12)\n",
    "    ax = fig.add_subplot(1, 2, 2)\n",
    "    ax.plot(train_loss,'-o', label='Train Loss')\n",
    "    ax.plot(val_loss,'--<', label='Validation Loss')\n",
    "    ax.set_title(\"Train-Validation Loss\",size=12)\n",
    "    ax.legend(loc='best',fontsize=12)\n",
    "    ax.set_xlabel('Epoch', size=12)\n",
    "    ax.set_ylabel('Loss', size=12)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 387
    },
    "executionInfo": {
     "elapsed": 1344,
     "status": "ok",
     "timestamp": 1671336065740,
     "user": {
      "displayName": "Elpina Fang",
      "userId": "06313423315987558468"
     },
     "user_tz": -420
    },
    "id": "yC0TMp22dFTI",
    "outputId": "9982d1df-7c98-4616-b668-3e46d0236720"
   },
   "outputs": [],
   "source": [
    "plot_learning_curves(train_acc, val_acc, train_loss, val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sYx1eghpdFTI"
   },
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Ybc8v2qB2W3"
   },
   "source": [
    "Selanjutnya akan ditampilkan evaluasi model melalui confusion matrix dan classification report dengan menggunakan eval_model function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1024,
     "status": "ok",
     "timestamp": 1671336073508,
     "user": {
      "displayName": "Elpina Fang",
      "userId": "06313423315987558468"
     },
     "user_tz": -420
    },
    "id": "7LyejJandFTI"
   },
   "outputs": [],
   "source": [
    "def eval_model(model):\n",
    "    y_act = []\n",
    "    y_pred= []\n",
    "    model.eval()\n",
    "    for data_t, target_t in (val_dl):\n",
    "        data_t, target_t = data_t.to(device), target_t.to(device)\n",
    "        outputs_t = model(data_t)\n",
    "        _,pred_t = torch.max(outputs_t, dim=1)\n",
    "        predictions = pred_t.to(\"cpu\")\n",
    "        y_pred.extend(predictions.numpy())\n",
    "        actual = target_t.to(\"cpu\")\n",
    "        y_act.extend(actual.numpy())\n",
    "    cf_matrix = confusion_matrix(y_act,y_pred)\n",
    "    sns.set_theme(rc={'figure.figsize':(8,8)})\n",
    "    ax = sns.heatmap(cf_matrix,annot=True,cmap='Reds',fmt=\"g\",xticklabels=train_dataset.classes,yticklabels=train_dataset.classes,cbar=False)\n",
    "    ax.set_ylabel('True Labels')\n",
    "    ax.set_xlabel('Predicted Labels');\n",
    "    plt.show()\n",
    "    print(classification_report(y_act,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 676
    },
    "executionInfo": {
     "elapsed": 10237,
     "status": "ok",
     "timestamp": 1671336093047,
     "user": {
      "displayName": "Elpina Fang",
      "userId": "06313423315987558468"
     },
     "user_tz": -420
    },
    "id": "VZd7IeqPdFTJ",
    "outputId": "807f1779-56f7-44b7-c625-3b0813e2cf48"
   },
   "outputs": [],
   "source": [
    "eval_model(resnet18_tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RN04lw__DQ6A"
   },
   "source": [
    "Dari output di atas dapat dilihat bahwa dari total 600 images pada data testing, sebanyak 593 images diklasifikasikan dengan tepat dan sisanya yaitu sebanyak 7 images mengalami misklasifikasi. Dari 7 image yang salah diklasikasikan tersebut, 3 images carrots diprediksi model merupakan image potatos,  2 images papayas diprediksi model merupakan image potatos, 1 images potatos diprediksi model merupakan image potatos, dan 1 images potatos diprediksi model merupakan image papayas. Dari confusion matrix tersebut, diperoleh akurasi, recall, dan precision masing-masing adalah 98%, 99%, dan 98%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1iI_C6QidFTK"
   },
   "source": [
    "## Tuning parameters Resnet18 Pretained Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ClBK6fFdFTK"
   },
   "source": [
    "Selanjutnya pada resnet18 pretained model, akan dilakukan tuning parameter terhadap learning rate, optimizer, dan loss function. Untuk melakukan hal tersebut, dibuat objective function untuk menentukan parameter yang akan dituning, memanggil train_model function dan mengembalikan skor evaluasinya (Objective value). Adapun library yg digunakan untuk tuning parameter adalah library optuna. Di dalam Objective Function, perlu mendefinisikan parameter yang ingin dioptimalkan. Dalam hal ini parameter yang dituning adalah :\n",
    "\n",
    "- learning rate : dari 1e-4 hingga 1e-2.\n",
    "- optimizer : SGD, Adam, dan Adadelta\n",
    "- Loss Function : CrossEntropyLoss dan NLLLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 994,
     "status": "ok",
     "timestamp": 1671336100359,
     "user": {
      "displayName": "Elpina Fang",
      "userId": "06313423315987558468"
     },
     "user_tz": -420
    },
    "id": "MacTW8qpdFTK"
   },
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    \n",
    "    # Hyperparameters we want optimize\n",
    "    params = {\n",
    "        \"loss_function\": trial.suggest_categorical('loss_function',[\"nn.CrossEntropyLoss()\",\"nn.NLLLoss()\"]),\n",
    "        \"learning_rate\": trial.suggest_loguniform('learning_rate', 1e-4, 1e-2),\n",
    "        \"optimizer_name\": trial.suggest_categorical('optimizer_name',['SGD', 'Adam', 'Adadelta'])\n",
    "    }\n",
    "    \n",
    "    # Get pretrained model\n",
    "    model = resnet18_tf\n",
    "    \n",
    "    # Define criterion\n",
    "    criterion = eval(params['loss_function'])\n",
    "    \n",
    "    # Configure optimizer\n",
    "    optimizer = getattr(\n",
    "        torch.optim, params[\"optimizer_name\"]\n",
    "    )(model.parameters(), lr=params[\"learning_rate\"])\n",
    "    \n",
    "    # Train model\n",
    "    best_model, best_acc = train_model(trial, model, criterion, optimizer, num_epochs=10)\n",
    "    \n",
    "    # Save best model for each trial\n",
    "    torch.save(best_model.state_dict(), f\"model_tf_trial_{trial.number}.pth\")\n",
    "    \n",
    "    return best_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 974,
     "status": "ok",
     "timestamp": 1671336104719,
     "user": {
      "displayName": "Elpina Fang",
      "userId": "06313423315987558468"
     },
     "user_tz": -420
    },
    "id": "CbfDXgMpdFTL"
   },
   "outputs": [],
   "source": [
    "def train_model(trial, model, criterion, optimizer, num_epochs):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        for phase in ['train', 'valid']:\n",
    "            if phase == 'train':\n",
    "                model.train() \n",
    "            else:\n",
    "                model.eval()  \n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            if phase == 'valid' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "        \n",
    "        trial.report(epoch_acc, epoch)\n",
    "        if trial.should_prune():\n",
    "            raise optuna.TrialPruned()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, best_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "davKBTwHFpuu"
   },
   "source": [
    "Untuk melakukan optimasi pada objective function, dicreate new study function yang sudah disediakan library optuna. untuk sampler menggunakan TPE sampler, pruner menggunakan MedianPruner untuk interrupt unpromising trialsnya, direction menggunakan maximize untuk memaksimalkan akurasi, dan n_trials adalah jumlah trials yang digunakan yaitu sebanyak 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 55232,
     "status": "ok",
     "timestamp": 1671343119099,
     "user": {
      "displayName": "Elpina Fang",
      "userId": "06313423315987558468"
     },
     "user_tz": -420
    },
    "id": "BDAzuwtidFTL",
    "outputId": "6b405000-7018-4f19-b314-a3b38e9cbb58"
   },
   "outputs": [],
   "source": [
    "sampler = optuna.samplers.TPESampler()    \n",
    "study = optuna.create_study(sampler=sampler, pruner=optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=5, interval_steps=5), direction='maximize')\n",
    "study.optimize(func=objective, n_trials=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NlNSQ0l3Gz3p"
   },
   "source": [
    "Dari output diatas dapat dilihat bahwa akurasi yang diperoleh adalah 99,67% pada trial ke 1 dengan best parameters :\n",
    "\n",
    "- loss_function : nn.CrossEntropyLoss()\n",
    "- learning_rate : 0.0028325557834021796\n",
    "- optimizer_name : Adadelta.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "executionInfo": {
     "elapsed": 532,
     "status": "ok",
     "timestamp": 1671343401778,
     "user": {
      "displayName": "Elpina Fang",
      "userId": "06313423315987558468"
     },
     "user_tz": -420
    },
    "id": "3MAs-2ukdFTM",
    "outputId": "658e972f-efe6-44e3-af86-57d280aeb8fa"
   },
   "outputs": [],
   "source": [
    "optuna.visualization.plot_parallel_coordinate(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "executionInfo": {
     "elapsed": 991,
     "status": "ok",
     "timestamp": 1671343423296,
     "user": {
      "displayName": "Elpina Fang",
      "userId": "06313423315987558468"
     },
     "user_tz": -420
    },
    "id": "wb3ZjXTxdFTM",
    "outputId": "71468777-30d4-4e8f-fd0c-2a25c6b10d61"
   },
   "outputs": [],
   "source": [
    "optuna.visualization.plot_contour(study, params=['learning_rate','optimizer_name','loss_function'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "executionInfo": {
     "elapsed": 939,
     "status": "ok",
     "timestamp": 1671343432352,
     "user": {
      "displayName": "Elpina Fang",
      "userId": "06313423315987558468"
     },
     "user_tz": -420
    },
    "id": "C0F1LafZdFTN",
    "outputId": "2e6cbd12-8a13-4196-a593-d2b7ef1de69e"
   },
   "outputs": [],
   "source": [
    "optuna.visualization.plot_slice(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "executionInfo": {
     "elapsed": 637,
     "status": "ok",
     "timestamp": 1671343439044,
     "user": {
      "displayName": "Elpina Fang",
      "userId": "06313423315987558468"
     },
     "user_tz": -420
    },
    "id": "UI-1vWDNdFTN",
    "outputId": "07f444d7-e397-4131-a1fe-76ee9f124d02"
   },
   "outputs": [],
   "source": [
    "optuna.visualization.plot_param_importances(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "executionInfo": {
     "elapsed": 1246,
     "status": "ok",
     "timestamp": 1671343448098,
     "user": {
      "displayName": "Elpina Fang",
      "userId": "06313423315987558468"
     },
     "user_tz": -420
    },
    "id": "-ej29PN5dFTO",
    "outputId": "39f3e949-969e-4835-fba1-30f33bfaa016"
   },
   "outputs": [],
   "source": [
    "optuna.visualization.plot_optimization_history(study)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3QmlOqjwJDTN"
   },
   "source": [
    "Berikut adalah best parameter dan best trial yang diperoleh, seperti yang dijelaskan sebelumnya."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 488,
     "status": "ok",
     "timestamp": 1671343454338,
     "user": {
      "displayName": "Elpina Fang",
      "userId": "06313423315987558468"
     },
     "user_tz": -420
    },
    "id": "NZYJZOvtdFTO",
    "outputId": "ef21c6a2-c695-441f-bae2-e564e993e19d"
   },
   "outputs": [],
   "source": [
    "print(\"Best Parameters :\", study.best_params)\n",
    "print(\"Best Best Trial Number :\", study.best_trial._number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 813,
     "status": "ok",
     "timestamp": 1671343462795,
     "user": {
      "displayName": "Elpina Fang",
      "userId": "06313423315987558468"
     },
     "user_tz": -420
    },
    "id": "zA_GxpK4dFTP",
    "outputId": "0def91f3-9e73-47e2-bc67-71b33c72265b"
   },
   "outputs": [],
   "source": [
    "resnet18_tf_tuned=resnet18_tf\n",
    "resnet18_tf_tuned.load_state_dict(torch.load(f'model_tf_trial_{study.best_trial._number}.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iY4CIwhsJekl"
   },
   "source": [
    "Dengan menggunakan best model yaitu dengan parameter {'loss_function': 'nn.CrossEntropyLoss()', 'learning_rate': 0.0028325557834021796, 'optimizer_name': 'Adadelta'} diperoleh confusion matrix dan classification report sebagai berikut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 676
    },
    "executionInfo": {
     "elapsed": 11616,
     "status": "ok",
     "timestamp": 1671343493060,
     "user": {
      "displayName": "Elpina Fang",
      "userId": "06313423315987558468"
     },
     "user_tz": -420
    },
    "id": "9FbTk9VWdFTP",
    "outputId": "c8e36f91-6179-4fb7-a49a-4b00a5e939b1"
   },
   "outputs": [],
   "source": [
    "eval_model(resnet18_tf_tuned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3KB4YFCEJsDN"
   },
   "source": [
    "Dari output di atas dapat dilihat bahwa dari total 600 images pada data testing, semua 600 images diklasifikasikan dengan tepat. Maka dari confusion matrix tersebut, diperoleh akurasi, recall, dan precision masing-masing adalah  100%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CYsuS00sdFTP"
   },
   "source": [
    "## B. Resnet18 from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 500,
     "status": "ok",
     "timestamp": 1671343499249,
     "user": {
      "displayName": "Elpina Fang",
      "userId": "06313423315987558468"
     },
     "user_tz": -420
    },
    "id": "9darUVzxdFTP"
   },
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, identity_downsample=None, stride=1):\n",
    "        super(Block, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.identity_downsample = identity_downsample\n",
    "        \n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        if self.identity_downsample is not None:\n",
    "            identity = self.identity_downsample(identity)\n",
    "        x += identity\n",
    "        x = self.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1004,
     "status": "ok",
     "timestamp": 1671343503798,
     "user": {
      "displayName": "Elpina Fang",
      "userId": "06313423315987558468"
     },
     "user_tz": -420
    },
    "id": "W6X86GaWdFTQ"
   },
   "outputs": [],
   "source": [
    "class ResNet_18(nn.Module):\n",
    "    \n",
    "    def __init__(self, image_channels, num_classes):\n",
    "        \n",
    "        super(ResNet_18, self).__init__()\n",
    "        self.in_channels = 32\n",
    "        self.conv1 = nn.Conv2d(image_channels, 32, kernel_size=7, stride=2, padding=3)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        #resnet layers\n",
    "        self.layer1 = self.__make_layer(32, 32, stride=1)\n",
    "        self.layer2 = self.__make_layer(32, 64, stride=2)\n",
    "        self.layer3 = self.__make_layer(64, 128, stride=2)\n",
    "        self.layer4 = self.__make_layer(128, 256, stride=2)\n",
    "        \n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(256, num_classes)\n",
    "        \n",
    "    def __make_layer(self, in_channels, out_channels, stride):\n",
    "        \n",
    "        identity_downsample = None\n",
    "        if stride != 1:\n",
    "            identity_downsample = self.identity_downsample(in_channels, out_channels)\n",
    "            \n",
    "        return nn.Sequential(\n",
    "            Block(in_channels, out_channels, identity_downsample=identity_downsample, stride=stride), \n",
    "            Block(out_channels, out_channels)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        \n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        \n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.fc(x)\n",
    "        return x \n",
    "    \n",
    "    def identity_downsample(self, in_channels, out_channels):\n",
    "        \n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=2, padding=1), \n",
    "            nn.BatchNorm2d(out_channels)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1001,
     "status": "ok",
     "timestamp": 1671343518116,
     "user": {
      "displayName": "Elpina Fang",
      "userId": "06313423315987558468"
     },
     "user_tz": -420
    },
    "id": "NXiha5LHdFTQ"
   },
   "outputs": [],
   "source": [
    "net_scracth = ResNet_18(3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1671343520892,
     "user": {
      "displayName": "Elpina Fang",
      "userId": "06313423315987558468"
     },
     "user_tz": -420
    },
    "id": "iG5oBF6xdFTQ",
    "outputId": "33f87291-f498-4a6b-fde5-b7c3e7b63f46"
   },
   "outputs": [],
   "source": [
    "#count trainable parameters of the model\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "count_parameters(net_scracth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 501,
     "status": "ok",
     "timestamp": 1671343524301,
     "user": {
      "displayName": "Elpina Fang",
      "userId": "06313423315987558468"
     },
     "user_tz": -420
    },
    "id": "SxZ_6ilpdFTQ",
    "outputId": "d32426c7-7575-421d-87c1-976c887404ad"
   },
   "outputs": [],
   "source": [
    "#move the model to the device\n",
    "net_scracth.to(device)\n",
    "next(net_scracth.parameters()).is_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 165
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "error",
     "timestamp": 1671343985046,
     "user": {
      "displayName": "Elpina Fang",
      "userId": "06313423315987558468"
     },
     "user_tz": -420
    },
    "id": "j6QVeIDgdFTQ",
    "outputId": "2edf6216-a0e4-44f2-cbcc-5902d9644c4c"
   },
   "outputs": [],
   "source": [
    "resnet18_scracths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l0aX79asLpf8"
   },
   "source": [
    "Mendefinisikan nilai epoch = 10 dengan menggunakan CrossEntropyLoss Function dan Optimizer Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "executionInfo": {
     "elapsed": 762,
     "status": "error",
     "timestamp": 1671344011698,
     "user": {
      "displayName": "Elpina Fang",
      "userId": "06313423315987558468"
     },
     "user_tz": -420
    },
    "id": "uTlcqG-ydFTR",
    "outputId": "8d8466bf-c178-4980-98b0-8ac09a6a2dfb"
   },
   "outputs": [],
   "source": [
    "#define everything we need for training\n",
    "epochs = 10\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net_scracth.parameters(), lr=0.0001, weight_decay=1e-4)\n",
    "lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aLodVxwIdFTR",
    "outputId": "6a5aca9d-ec2f-488e-a133-3499b36e1ac4"
   },
   "outputs": [],
   "source": [
    "n_epochs = 10\n",
    "print_every = 10\n",
    "valid_loss_min = np.Inf\n",
    "val_loss = []\n",
    "val_acc = []\n",
    "train_loss = []\n",
    "train_acc = []\n",
    "total_step = len(train_dl)*3\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total=0\n",
    "    print(f'Epoch {epoch}\\n')\n",
    "    for batch_idx, (data_, target_) in enumerate(train_dl):\n",
    "        data_, target_ = data_.to(device), target_.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net_scracth(data_)\n",
    "        loss = criterion(outputs, target_)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _,pred = torch.max(outputs, dim=1)\n",
    "        correct += torch.sum(pred==target_).item()\n",
    "        total += target_.size(0)\n",
    "        if (batch_idx) % 20 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch, n_epochs, batch_idx, total_step, loss.item()))\n",
    "    train_acc.append(100 * correct / total)\n",
    "    train_loss.append(running_loss/total_step)\n",
    "    print(f'\\ntrain-loss: {np.mean(train_loss):.4f}, train-acc: {(100 * correct/total):.4f}')\n",
    "    batch_loss = 0\n",
    "    total_t=0\n",
    "    correct_t=0\n",
    "    with torch.no_grad():\n",
    "        net_scracth.eval()\n",
    "        for data_t, target_t in (val_dl):\n",
    "            data_t, target_t = data_t.to(device), target_t.to(device)\n",
    "            outputs_t = net_scracth(data_t)\n",
    "            loss_t = criterion(outputs_t, target_t)\n",
    "            batch_loss += loss_t.item()\n",
    "            _,pred_t = torch.max(outputs_t, dim=1)\n",
    "            correct_t += torch.sum(pred_t==target_t).item()\n",
    "            total_t += target_t.size(0)\n",
    "        val_acc.append(100 * correct_t/total_t)\n",
    "        val_loss.append(batch_loss/len(val_dl))\n",
    "        network_learned = batch_loss < valid_loss_min\n",
    "        print(f'validation loss: {np.mean(val_loss):.4f}, validation acc: {(100 * correct_t/total_t):.4f}\\n')\n",
    "        \n",
    "        if network_learned:\n",
    "            valid_loss_min = batch_loss\n",
    "            torch.save(net_scracth.state_dict(), 'resnet_sc.pt')\n",
    "            print('Improvement-Detected, save-model')\n",
    "    net_scracth.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8h7gGVLXdFTR"
   },
   "outputs": [],
   "source": [
    "plot_learning_curves(train_acc, val_acc, train_loss, val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vfRH-v2ZdFTS"
   },
   "outputs": [],
   "source": [
    "eval_model(net_scracth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BV0oTJmFdFTS"
   },
   "source": [
    "## Tuning parameters Resnet18 From Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D8bSODr4dFTS"
   },
   "source": [
    "Seperti tuning parameter pada resnet18 pretained model sebelumnya, tuning parameter juga akan dilakukan pada resnet18 (scratch) terhadap learning rate, optimizer, dan loss function. Untuk melakukan hal tersebut, dibuat objective function untuk menentukan parameter yang akan dituning, memanggil train_model function yang sudah digenerate pada bagian sebelumnya dan mengembalikan skor evaluasinya (Objektive value/akurasi). Adapun library yg digunakan untuk tuning parameter adalah library optuna. Di dalam Objective Function, perlu mendefinisikan parameter yang ingin dioptimalkan. Dalam hal ini parameter yang dituning adalah :\n",
    "\n",
    "- learning rate : dari 1e-4 hingga 1e-2.\n",
    "- optimizer : SGD, Adam, dan Adadelta\n",
    "- Loss Function : CrossEntropyLoss dan NLLLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I9ONMczydFTS"
   },
   "outputs": [],
   "source": [
    "def objective_v2(trial):\n",
    "    \n",
    "    # Hyperparameters we want optimize\n",
    "    params = {\n",
    "        \"loss_function\": trial.suggest_categorical('loss_function',[\"nn.CrossEntropyLoss()\",\"nn.NLLLoss()\"]),\n",
    "        \"learning_rate\": trial.suggest_loguniform('learning_rate', 1e-4, 1e-2),\n",
    "        \"optimizer_name\": trial.suggest_categorical('optimizer_name',['SGD', 'Adam', 'Adadelta'])\n",
    "    }\n",
    "    \n",
    "    model = net_scracth\n",
    "    # Define criterion\n",
    "    criterion = eval(params['loss_function'])\n",
    "    \n",
    "    # Configure optimizer\n",
    "    optimizer = getattr(\n",
    "        torch.optim, params[\"optimizer_name\"]\n",
    "    )(model.parameters(), lr=params[\"learning_rate\"])\n",
    "    \n",
    "    # Train model\n",
    "    best_model, best_acc = train_model(trial, model, criterion, optimizer, num_epochs=10)\n",
    "    \n",
    "    # Save best model for each trial\n",
    "    torch.save(best_model.state_dict(), f\"model_scr_trial_{trial.number}.pth\")\n",
    "    \n",
    "    return best_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1IgSKG0idFTS"
   },
   "outputs": [],
   "source": [
    "sampler = optuna.samplers.TPESampler()    \n",
    "study_scratch = optuna.create_study(\n",
    "    sampler=sampler,\n",
    "    pruner=optuna.pruners.MedianPruner(\n",
    "        n_startup_trials=5, n_warmup_steps=5, interval_steps=5\n",
    "    ),\n",
    "    direction='maximize')\n",
    "study_scratch.optimize(func=objective_v2, n_trials=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7xdQSBLBdFTT"
   },
   "outputs": [],
   "source": [
    "optuna.visualization.plot_parallel_coordinate(study_scratch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q_A0qoj9dFTT"
   },
   "outputs": [],
   "source": [
    "optuna.visualization.plot_contour(study_scratch, params=['learning_rate','optimizer_name','loss_function'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3LVM8x39dFTT"
   },
   "outputs": [],
   "source": [
    "optuna.visualization.plot_slice(study_scratch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h21vLzZpdFTU"
   },
   "outputs": [],
   "source": [
    "optuna.visualization.plot_param_importances(study_scratch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aXzcNOA1dFTU"
   },
   "outputs": [],
   "source": [
    "optuna.visualization.plot_optimization_history(study_scratch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jGl5eRC_dFTU"
   },
   "outputs": [],
   "source": [
    "print(\"Best Parameters :\", study_scratch.best_params)\n",
    "print(\"Best Best Trial Number :\", study_scratch.best_trial._number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aBiYlskydFTU"
   },
   "outputs": [],
   "source": [
    "net_scracth_tuned=net_scracth\n",
    "net_scracth_tuned.load_state_dict(torch.load(f'model_scr_trial_{study_scratch.best_trial._number}.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FeCAxl9KdFTV"
   },
   "outputs": [],
   "source": [
    "eval_model(net_scracth_tuned)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
